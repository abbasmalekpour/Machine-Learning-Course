{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h1 align=\"center\">Assignment 2: Logistic Regression</h1>\n",
    "     <h3 align=\"center\">Machine Learning Course </h3>\n",
    "    <h5 align=\"center\">Abbas Malekpour <a href='/'>ML</a></h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the contents in this assigment and the following assignments are heavily inspired from the amazing machine learning course presented at Stanford by Andrew Ng.\n",
    "\n",
    "However, here in this series assignments for our machine learning course, we have used **Python** language which we think is much more suitable than Matlab for artificial intelligence students. Also we have changed and modified some parts of the assignments and the related descriptions for those parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you only need to implement the following functions:\n",
    "- `sigmoid(x)`: \n",
    " - this is a warmup exercize and not graded.\n",
    "- `logistic_regression_cost(theta, X, y)`: \n",
    " - cost function for logistic regression without regularization.\n",
    "- `grads(theta, X, y)`: \n",
    " - the gradients of the above function.\n",
    "- `predict(theta, X):`: \n",
    " - prediction function which predicts the label or class of the input data.\n",
    "- `regularized_logistic_regression_cost(theta, X, y, reg)`: \n",
    " - cost function for logistic regression with regularization.\n",
    "- `regularized_grads(theta, X, y, reg)`: \n",
    " - the gradients of the above function.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import two useful python library which we will use through this assinment.\n",
    "- The first library is `numpy` to work with N-dimensional arrays.\n",
    "- The second library is `matplotlib` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# setup for printing numpy arrays\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this exercise, we'll build a logistic regression model to predict whether a student gets admitted to a university."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you are the administrator of a university department and you want to determine each applicant's chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant's scores on two exams and the admissions decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that you need for this part of the assignment is given to you in the text file `data/ex2data1.txt`. Here, you can see the first 10 rows from this file. The first two columns or the **input features** are the applicant's scores on two different exams and the third column (or the **target output**) is the admissions decesion. Here, `0` means \"Not Admitted\" and `1` means \"Admitted\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "34.62365962451697,78.02469281536240,0\n",
    "30.28671076822607,43.89499752400101,0\n",
    "35.84740876993872,72.90219802708364,0\n",
    "60.18259938620976,86.30855209546826,1\n",
    "79.03273605071010,75.34437643691030,1\n",
    "45.08327747668339,56.31637178153050,0\n",
    "61.10666453684766,96.51142588489624,1\n",
    "75.02474556738889,46.55401354116538,1\n",
    "76.09878670226257,87.42056971926803,1\n",
    "84.43281996120035,43.53339331072109,1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our goal is to build a classification model that estimates the probability of admission based on the exam scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `load_data()` has already been set up to load this data for you. It recieves the file name which contains the data (in this case `data/ex2data1.txt`) and returns `X` and `y`. This function is implemented for you in the file `utils.py` and it is imported at the begining of this notebook using the following `import` statement.\n",
    "```python\n",
    "    from utils import *\n",
    "```\n",
    " Please see the implementation and make sure that you understand every detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(fname='data/ex2data1.txt', include_bias=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that unlike the previous assignment, here in this assignment `y` is a 1d vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (`profit` and `population`). (Many other problems that you will encounter in real life are multi-dimensional and can’t be plotted on a 2-d plot.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can call `plot_data` to create a scatter plot of the data. Again, we have implemented this function for you and imported it at the begining of this notebook. Run the following cell to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, y, labels=['Not Admitted', 'Admitted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup exercize: sigmoid function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start with the actual cost function, recall that the logistic regression hypothesis is deﬁned as:\n",
    "$$h_\\theta(x)=g(\\theta^T x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where function g is the sigmoid function. The sigmoid function is deﬁned as:\n",
    "$$g(z)=\\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first step is to implement this function so it can be called by the rest of your program. When you are finished, try testing a few values by calling `sigmoid(x)`. For large positive values of `x`, the sigmoid should be close to `1`, while for large negative values, the sigmoid should be close to `0`. Evaluating `sigmoid(0)` should give you exactly `0.5`. \n",
    "\n",
    "Note that your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        - x: input, can be a scaler value or a numpy array\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "        - The sogmoid of x. \n",
    "        \n",
    "    Examples:\n",
    "    --------\n",
    "        - sigmoid(0)    -> 0.5\n",
    "        - sigmoid(100)  -> 1.0\n",
    "        - sigmoid(-100) -> 0.0\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"You need to implement `sigmoid` function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can run the following cells to see if your implementation of sigmoid function is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing sigmoid for scaler values\n",
    "print(\"sigmoid(   0) = %.2f\" % sigmoid(0))\n",
    "print(\"sigmoid( 100) = %.2f\" % sigmoid(100))\n",
    "print(\"sigmoid(-100) = %.2f\" % sigmoid(-100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check to make sure the implementation of sigmoid function is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10, 0.01)\n",
    "plt.plot(x, sigmoid(x), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function and gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement the cost function and gradient for logistic regression. Complete the code in `logistic_regression_cost()` to return the cost and the `grads()` to return the gradient of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the cost function in logistic regression is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)} \\log(h_\\theta(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the gradient of the cost is a vector θ where the jth element (for j = 0,1,...,n) is defined as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_cost(theta, X, y):\n",
    "    \"\"\" Vectorized implementation of logistic regression cost function.\n",
    "    \n",
    "    Parameters:\n",
    "        - theta: Parameters, a 1d array of shape (n + 1,)\n",
    "        - X: Inputs, a 2d array of shape (m, n + 1)\n",
    "        - y: Target values, a 1d array of shape (m,)\n",
    "        \n",
    "    Returns:\n",
    "        - logistic regression cost\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"You need to implement `logistic_regression_cost` function.\")\n",
    "\n",
    "def grads(theta, X, y):\n",
    "    \"\"\" Vectorized implementation of the gradient of logistic regression cost function.\n",
    "    \n",
    "    Parameters:\n",
    "        - theta: Parameters, a 1d array of shape (n + 1,)\n",
    "        - X: Inputs, a 2d array of shape (m, n + 1)\n",
    "        - y: Target values, a 2d array of shape (m,)\n",
    "        \n",
    "    Returns:\n",
    "        - gradient of the logistic regression cost function, a 2d array of shape (n + 1,)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"You need to implement `grads` function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your implementation of cost function, run the following cell. If you have implemented the cost function correctly, you should see that the cost is about `0.693` and gradient vector is about `[-0.1, -12.01, -11.26]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing cost function and gradient\n",
    "theta = np.zeros((3,))\n",
    "print(\"cost = %.3f\" % logistic_regression_cost(theta, X, y))\n",
    "print(\"gradient = %s\" % grads(theta, X, y).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters using `scipy.optimize.minimize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous assignment, you found the optimal parameters of a linear regression model by implementing gradent descent. You wrote a cost function and calculated its gradient, then took a gradient descent step accordingly. This time, instead of taking gradient descent steps, you will use a function in `scipy.optimize` libray called `minimize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us first import this function to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy.optimize.minimize` is an optimization solver that finds the minimum of a function. For logistic regression, you want to optimize the cost function `J(θ)` with parameters `θ`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, you are going to use `minimize` to find the best parameters `θ` for the logistic regression cost function, given a fixed dataset (of `X` and `y` values). You will pass to `minimize` the following inputs:\n",
    "- **Initial values**: The initial values of the parameters we are trying to optimize.\n",
    "- **Cost function**: A function that, when given the training set and a particular `θ`, computes the logistic regression cost for the dataset (`X`, `y`).\n",
    "- **Gradients**: A function, when given the training set and a particular `θ`, computes gradient with respect to `θ` for the dataset (`X`, `y`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "minimize(cost_function, x0=theta0, args=(X, y), method='CG', jac=grads)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = minimize(logistic_regression_cost, x0=np.zeros((3,)), args=(X, y), method='bfgs', jac=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_opt = result.x  # optimal theta\n",
    "print(theta_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will call your cost function using the optimal parameters of θ. You should see that the cost is about `0.203`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = logistic_regression_cost(theta_opt, X, y)\n",
    "print(\"cost = {:.3f}\".format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final `θ` value will then be used to plot the decision boundary on the training data, resulting in a figure similar to Figure 2. We also encourage you to look at the code in `plot_decision_boundary()` to see how to plot such a boundary using the `θ` values. This function is implemented for you in the file `utils.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is ok, you should see something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/part_1_log_reg_decision_boundary.png' width='75%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(theta_opt, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of `45` and an Exam 2 score of `85`, you should expect to see an admission probability of `0.776`. \n",
    "\n",
    "Another way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. In this part, your task is to complete the code in `predict()`. The predict function will produce `1` or `0` predictions given a dataset and a learned parameter vector `θ`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    raise NotImplementedError(\"You need to implement `predict` function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([[1, 45, 85]])\n",
    "print(sigmoid(x_test @ theta_opt.T))\n",
    "print(predict(theta_opt, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two diﬀerent tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous part of this exercise, `plot_data()` is used to generate a figure like Figure 3, where the axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with different markers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(fname='data/ex2data2.txt')\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8), dpi=120)\n",
    "plot_data(X, y, xlabel='Microchip Test 1', ylabel='Microchip Test 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data looks a bit more complicated than the previous example. In particular, you'll notice that there is no linear decision boundary that will perform well on this data. One way to deal with this using a linear technique like logistic regression is to construct features that are derived from polynomials of the original features. \n",
    "\n",
    "Let's start by creating a bunch of polynomial features using the `map_features()` function that we have implented for you in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 8\n",
    "X_poly = map_features(X[:, 1], X[:, 2], degree=degree)\n",
    "print(X_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to modify the cost and gradient functions from part 1 to include the regularization term. Please complete the implementation of the two following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_logistic_regression_cost(theta, X, y, reg):\n",
    "    \"\"\" Vectorized implementation of logistic regression cost function.\n",
    "    \n",
    "    Parameters:\n",
    "        - theta: Parameters, a 1d array of shape (n + 1,)\n",
    "        - X: Inputs, a 2d array of shape (m, n + 1)\n",
    "        - y: Target values, a 1d array of shape (m,)\n",
    "        \n",
    "    Returns:\n",
    "        - logistic regression cost\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"You need to implement `regularized_logistic_regression_cost` function.\")\n",
    "\n",
    "\n",
    "def regularized_grads(theta, X, y, reg):\n",
    "    \"\"\" Vectorized implementation of the gradient of logistic regression cost function.\n",
    "    \n",
    "    Parameters:\n",
    "        - theta: Parameters, a 1d array of shape (n + 1,)\n",
    "        - X: Inputs, a 2d array of shape (m, n + 1)\n",
    "        - y: Target values, a 2d array of shape (m,)\n",
    "        \n",
    "    Returns:\n",
    "        - gradient of the logistic regression cost function, a 2d array of shape (n + 1,)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"You need to implement `regularized_grads` function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have implented these two function correctly, you should see that for $\\theta$ equals to zero, the cost is about `0.693`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X_poly.shape[1])\n",
    "cost = regularized_logistic_regression_cost(theta, X_poly, y, 1.0)\n",
    "print('cost = {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the same optimization function from part 1 to compute the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 0.0\n",
    "result = minimize(\n",
    "    regularized_logistic_regression_cost, \n",
    "    x0=theta, \n",
    "    args=(X_poly, y.ravel(), reg), \n",
    "    method='bfgs', \n",
    "    jac=regularized_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data. After running the following cell, you should see that the accuracy is about `88.14%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return 100. * np.sum(y_pred == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_opt = result.x\n",
    "predictions = predict(theta_opt, X_poly)\n",
    "acc = accuracy(predictions, y)\n",
    "print('accuracy = {:.2f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see the classification result. If everything is Ok, you should see something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/part_2_lambda_0.png' width='75%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a small λ, you should find that the classiﬁer gets almost every training example correct, but draws a very complicated boundary, thus **overfitting** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Degree = {}, $\\lambda$ = {}, accuracy = {:.2f}\".format(degree, reg, acc)\n",
    "plot_decision_boundary(theta_opt, X_poly, y.ravel(), \n",
    "                       xlabel='Microchip Test 1', ylabel='Microchip Test 2', \n",
    "                       degree=degree, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now by trying different value for `degree` and `reg` (regularization strength), find a an accurate and smooth decision boundary for the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the value of these two hyper-parameters\n",
    "degree = 8\n",
    "reg = 100.0\n",
    "\n",
    "# create polynomial features\n",
    "X_poly = map_features(X[:, 1], X[:, 2], degree=degree)\n",
    "\n",
    "# initialize theta to 0\n",
    "theta = np.zeros(X_poly.shape[1])\n",
    "\n",
    "# optimize cost function to find the optimal parameters\n",
    "result = minimize(\n",
    "    regularized_logistic_regression_cost, \n",
    "    x0=theta, \n",
    "    args=(X_poly, y.ravel(), reg), \n",
    "    method='bfgs', \n",
    "    jac=regularized_grads)\n",
    "\n",
    "# compute accuracy\n",
    "theta_opt = result.x\n",
    "predictions = predict(theta_opt, X_poly)\n",
    "acc = accuracy(predictions, y)\n",
    "\n",
    "# plot decision boundary\n",
    "title = \"Degree = {}, $\\lambda$ = {}, accuracy = {:.2f}\".format(degree, reg, acc)\n",
    "plot_decision_boundary(theta_opt, X_poly, y.ravel(), \n",
    "                       xlabel='Microchip Test 1', ylabel='Microchip Test 2',\n",
    "                       degree=degree, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we implemented these algorithms from scratch, it's worth noting that we could also use a high-level python library like scikit-learn to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "C = 1e10\n",
    "model = linear_model.LogisticRegression(penalty='l2', C=C, fit_intercept=False)\n",
    "model.fit(X_poly, y.ravel());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.score(X_poly, y) * 100\n",
    "\n",
    "theta_opt = np.array(model.coef_)\n",
    "\n",
    "title = \"Degree = {}, C = {}, accuracy = {:.2f}\".format(degree, C, acc)\n",
    "plot_decision_boundary(theta_opt, X_poly, y.ravel(), \n",
    "                       xlabel='Microchip Test 1', ylabel='Microchip Test 2',\n",
    "                       degree=degree, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submittion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the assignment, please send **only** this jupyter notebook file via Piazza as a **private** post before the due date. Before submitting your file, be sure to read the following terms and conditions.\n",
    "\n",
    "**Due date**: November, 9, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms and Conditions\n",
    "Please consider the following terms and conditions and take them as serious as you can:\n",
    "- If you send your code as a public code in Piazza, you will fail in the course.\n",
    "- If you share all or some parts of your results on the internet or with your friends and classmates in any form, you will fail the course.\n",
    "- If you submit codes that is not your work, (for example by copying your friend's codes), you will fail the course.\n",
    "- Any other attempt to cheat, will result in a failure in the course.\n",
    "- By submmiting this file to Piazza, you confirm that you have read and accepted the above terms and conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
